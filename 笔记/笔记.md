# 笔记

## 什么是语音克隆，voice clone?

学界将voice clone限制在TTS领域，也就是个性化语音合成 personalized speech synthesis.

## 论文1——面向小样本的个性化中文语音合成研究

一个概念是voice conversion，也有它的应用领域，比如将某首歌转换成某人的音色唱的。
我们从输入输出的角度来理解各个模型、技术。
语音合成如图：

### 一、语音合成

从功能上,可以把目前主流的语音合成的整体流程划分为文本特征提取、音频参数合成以及波形恢复三个部分。
目前,语音合成系统的实现主要有两种技术路线,分别是基于波形拼接和基于统计  参数合成。基于波形拼接的语音合成系统首先需要搭建一个庞大的语音数据库。基于统计参数合成的语音合成系统主要分为前端文本处理模块、声学模型和声码器三大部分。后来,研究人员们将深度学习技术在语音合成领域的前端处理、声学建模、  声码器等部件上进行了应用,并尝试使用神经网络端到端地对整个语音合成过程进行建  模,大大提高了语音合成系统的质量

## 二、语音克隆

在个性化语音克隆领域，研究者们通过不断改进模型结构和训练策略，逐步提高了语音克隆的质量和效率。以下是对各个语音克隆方案的详细说明及其改进的本质分析：

### 1.多说话人语音合成

- **Deep Voice 2** 和 **Deep Voice 3** 是最早的基于深度学习的多说话人语音合成系统之一。它们使用**说话人嵌入特征（Speaker Embedding）**来表示不同说话人的音色。这些嵌入特征是模型的可学习参数，与整个语音合成网络一起训练。
- **改进点**：这些系统能够高质量地还原训练集中各个说话人的音色，但无法直接生成训练集中未出现过的说话人音色。为了生成新说话人的音色，通常需要对模型进行微调。

### 2. **模型微调方案**

- **Chen 等人**的研究表明，使用目标说话人的少量数据（如数分钟的语音）对整个模型进行微调，可以有效复刻目标说话人的音色。他们尝试了不同微调策略，包括微调说话人嵌入特征、说话人编码器以及整个语音合成模型。
- **Moss 等人**提出的 **BOFFIN TTS** 系统在微调时固定了文本特征提取部分的参数，仅调整频谱参数合成部分的网络，从而减少了需要微调的参数量。
- **Arik 等人**则进一步简化了微调过程，仅微调说话人嵌入特征的参数。
- **AdaSpeech** 在解码器中引入了**条件层归一化（Conditional Layer Normalization）**，利用说话人嵌入特征预测层归一化的平移因子和缩放因子。在进行目标说话人适应微调时，仅调整说话人嵌入特征和条件层归一化的参数。

**改进点**：：这些方法通过减少微调的参数量，缓解了目标说话人数据短缺的问题。然而，微调过程仍然需要一定的数据量和计算资源，且微调步骤较为繁琐。

### 3. **SVTTS（无需微调的自适应语音克隆）**

![alt text](image.png)

- **Jia 等人**提出的 **SVTTS** 系统引入了**自适应的说话人编码器**，省去了微调步骤。该系统首先基于声纹识别任务训练一个说话人编码器，能够为任意音频生成说话人嵌入特征。在系统使用阶段，只需输入目标说话人的音频，即可生成目标说话人的嵌入特征，并将其输入到 **Tacotron2** 模型中生成目标音色的语音。
- **改进点**：SVTTS 的关键在于说话人编码器的泛化能力。通过使用 **GE2E 损失**，系统能够使不同说话人的嵌入特征在特征空间中尽量远离，而同一说话人的嵌入特征尽量接近，从而提高模型的泛化性。这种方法无需微调，能够通过少量目标说话人数据生成高质量的语音克隆。

#### **基于 i-vector 和 x-vector 的改进**

- **Yang 等人**使用声纹识别领域的 **i-vector** 作为自适应的说话人嵌入特征。
- **Cooper 等人**在 **x-vector** 的基础上引入了可学习的字典编码方法，进一步泛化了说话人嵌入特征。
- **改进点**：这些方法通过引入声纹识别领域的先进技术，提升了说话人嵌入特征的泛化能力。然而，这些嵌入特征通常是句子级别的，未能充分利用目标说话人的细颗粒音色细节。

### 作者的研究——音素级别的音色特征提取——建立新的音素库

基于目标说话人的一段语音，也称之为参考语音，d-vector、x-vector 等说话人表征方法可以为其产生句子级别的说话人嵌入特征，但此类说话人嵌入特征往往是对参考语音数据的平均化表征，即使可以从整体上表示说话人的音色特征，但仍损失了许多句子中的细节音色特征。
本节提出的说话人编码器将提取细颗粒的说话人音色特征，并从语句中可独立发声的最小单位——音素中对细颗粒的说话人特征进行归结，得到音素级别的说话人特征。
对于不存在于参考语音中的音素，我们提出一个基于注意力机制的音素级别说话人特征迁移，通过参考语音中已有音素的说话人特征，求得未现音素的说话人特征。

---

## 论文2——Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis

**被引千次的大牛级别。**
**提出SVTTS**，也就是利用speaker verification 的任务训练一个speaker encoder，得到音色特征向量d-vector.
将d-vector与文本特征向量结合作为合成器的输入。
![alt text](image.png)

### **1. 说话人编码器：提取说话人特征**

说话人编码器用于从目标说话人的参考语音信号中提取一个固定维度的表示（称为**d-vector**），以捕获说话人的独特特征。这个d-vector作为合成网络的条件输入。

#### 关键点

- **输入**：从任意长度的语音片段计算得到的log-mel频谱帧序列。
- **输出**：一个固定维度的嵌入向量（d-vector），表示说话人的身份。
- **训练目标**：
  - 说话人编码器在**与文本无关的说话人验证任务**上进行训练。
  - 目标是优化一个端到端的说话人验证损失函数，使得：
    - 同一说话人的不同语音片段的嵌入向量具有高余弦相似度。
    - 不同说话人的语音片段的嵌入向量在嵌入空间中距离较远。
- **架构**：
  - 输入：40通道的log-mel频谱图。
  - 网络：3层LSTM堆叠，每层768个单元，后面接一个投影层，将维度降到256。
  - 最终嵌入：顶层最后一帧的输出经过L2归一化得到。
- **推理过程**：
  - 对于任意长度的语音片段，将其分割为重叠的800毫秒窗口（50%重叠）。
  - 网络独立处理每个窗口，最后将所有窗口的输出取平均并归一化，生成最终的语音嵌入。

#### 为什么有效

尽管说话人编码器并未直接针对语音合成任务优化，但通过说话人区分任务学到的嵌入向量可以直接用于合成网络中的说话人身份条件化。

### **2. 合成器：结合说话人嵌入与文本特征**

合成器扩展了Tacotron 2架构，支持多说话人合成，通过将说话人嵌入向量引入合成过程。

#### 关键点

- **文本表示**：
  - 输入文本被映射为音素序列，这有助于加速收敛并改善罕见单词和专有名词的发音准确性。
- **说话人嵌入整合**：
  - 在每个时间步，说话人嵌入向量与编码器输出（文本特征向量）进行拼接。
  - 具体来说，在推理过程中，使用预训练的说话人编码器从目标音频中提取说话人嵌入，并将其传递给合成器的注意力层。
- **两种模型变体**：
  1. **使用说话人编码器**：
     - 在训练过程中，使用预训练的说话人编码器（参数冻结）从目标音频中提取说话人嵌入。
     - 训练时不使用显式的说话人标识标签。
  2. **基线查找表**：
     - 为训练集中每个说话人优化一个固定的嵌入向量，类似于[8, 13]中的方法。

### 3.如何将说话人嵌入与文本特征结合

- **拼接**：
  - 在每个时间步，将说话人嵌入向量与文本特征向量拼接在一起。
  - 这个组合后的特征向量随后被传递给注意力机制和合成器的后续层。
- **注意力层条件化**：
  - 与某些其他方法（例如[8]）不同，仅仅将说话人嵌入传递给注意力层就足以使模型在不同说话人之间收敛。

#### 其他方法（不在本论文中出现）

（1）**直接拼接（Concatenation）**

- 将音色嵌入向量与每个时间步的文本特征向量直接拼接在一起，形成一个新的特征向量。
- 示例公式：
     \[
     \text{Combined Feature} = [\text{Text Feature}; \text{Speaker Embedding}]
     \]
- 这种方法简单直观，但可能会导致维度膨胀问题。

（2）**加性融合（Additive Fusion）**

- 将音色嵌入向量广播到所有时间步，并与文本特征向量相加。
- 示例公式：
     \[
     \text{Combined Feature} = \text{Text Feature} + \text{Speaker Embedding}
     \]
- 这种方法能够保持原始特征的维度不变，同时让音色信息融入到每个时间步的文本特征中。

（3）**注意力机制（Attention Mechanism）**

- 使用多头注意力机制（Multi-head Attention）动态地调整音色嵌入向量与文本特征向量之间的权重。
- 具体来说，可以通过自注意力机制（Self-Attention）计算音色嵌入向量与文本特征向量的相关性，从而生成一个加权后的综合特征。
- 这种方法更加灵活，能够根据上下文动态调整音色的影响程度。

（4）**条件化生成（Conditional Generation）**

- 在某些SVTTS系统中，音色嵌入向量被用作条件输入，影响整个生成过程。
- 例如，在基于Transformer的架构中，音色嵌入向量可以作为额外的条件输入，注入到解码器的每一层中。
- 这种方法使得音色信息在整个生成过程中持续发挥作用。

---

## 论文3——Voice Cloning Using Artificial Intelligence and Machine Learning: A Review

印度老哥发表在动物学上的，他自己罗列了19-22年的几篇文献，但是自己却是23年12月发表，时效性略差。

### **1. 文献综述 (Literature Survey)**

配合后文的方法步骤，分别找了几篇

- 主要涵盖以下几个方面：
  - **背景噪声适应**：如何在嘈杂环境中保持语音质量（如[Jixun Yao et al., 2022]）。
  - **情感合成**：将情感融入合成语音以提高用户参与度（如情感识别模型的应用）。
  - **多说话人语音克隆**：支持多种说话人声音的生成。
  - **深度学习框架**：使用HiFi-GAN、Tacotron等先进模型生成高质量语音。
- 每个条目都提供了具体的指标（如MOS、PESQ、SISDR）来评估模型性能，并讨论了潜在的技术限制（如资源密集型训练、伦理问题等）。

### **2. 现有算法和技术 (Existing System And Algorithm)**

- **目的**：详细介绍实现语音克隆所依赖的主要算法和技术。
- **内容**：
  - 列举并解释了几种关键算法：
    - **Advanced HiFi-GAN模型**：用于生成高保真语音。***常用声码器***
    - **基于深度学习的情感识别模型**：用于检测和表达情感。***情感模块***
    - **序列到序列模型（Sequence-to-Sequence Models）**：将输入文本映射到输出语音。***文本特征提取***
    - **瓶颈特征提取模块（Bottleneck Feature Extraction Module）**：从音频流中提取重要特征。***音色特征提取***
    - **声音分离模块（Sound Separation Module）**：从输入音频中分离出背景噪声和语音信号。***声音材料预处理（降噪和后续从新加上噪声）***
    - **语音转换模块（Voice Conversion Module）**：调整语音特征以匹配目标说话人或情感。***未知**??*
  - 这部分还说明了这些模块如何协同工作，以实现高质量的语音合成。

### **3.基本步骤**

- **目的**：描述构建语音克隆模型的具体步骤和方法。
- **内容**：
  - 数据收集与预处理：组装包含多种情绪和说话风格的多样化文本和音频数据集，并对其进行清洗和标注。
  - 文本转语音（TTS）转换：使用先进的深度学习方法（如Tacotron、WaveNet）将文本转换为逼真的语音。
  - 声音转换与定制化生成：通过变分自编码器（VAE）生成潜变量表示，从而创建独特的个性化声音。
  - 情感注入（Emotion Infusion）：利用自然语言处理（NLP）技术分析输入文本中的情感，并调整语音的韵律、语调和节奏以匹配指定情绪。
  - 背景噪声生成：开发声音分离模块以分离背景噪声，并将其重新叠加到合成语音上，确保整体音频质量。

---

## 论文4——A Survey on Neural Speech Synthesis

### 1. 风格化/个性化语音合成（语音克隆）

在 “Adaptive TTS”（适应性TTS）部分有详细讨论，特别是在 “Efficient Adaptation”（高效适应）小节中，提到了如何利用少量数据和参数来适应目标说话者的声音。
**相关原理：**
AdaSpeech [40]：提出了一种自适应文本到语音（AdaSpeech）的方法，通过条件层归一化（conditional layer normalization）生成说话者相关的参数，从而实现对目标说话者声音的高效适应。
Zero-Shot Adaptation（零样例适应）：一些工作 [9, 44, 142, 56] 探讨了如何在没有目标说话者数据的情况下，通过预训练的说话者编码器提取目标说话者的声音特征，实现零样例的语音克隆。
Few-Shot Adaptation（少样例适应）：一些工作 [44, 9, 177, 240, 446, 49, 40, 236] 探讨了如何利用少量的目标说话者数据（几句话甚至几秒的音频）来适应目标说话者的声音。
Untranscribed Data Adaptation（无转录数据适应）：AdaSpeech 2 [403] 提出了利用无转录的语音数据进行语音克隆的方法，通过语音重建和潜在对齐（latent alignments）实现对目标说话者声音的适应。

### 2. 文献组织顺序——语音合成基本步骤的演变

- 1.早期方法：
Articulatory Synthesis：通过模拟人类发音器官（如嘴唇、舌头、声带）的运动来生成语音。这种方法理论上最接近人类发音，但实现难度大，数据获取困难。
Formant Synthesis：基于规则的共振峰模型，通过控制共振峰和基频等参数生成语音。这种方法生成的语音可懂度高，但自然度较差。
Concatenative Synthesis：依赖于存储的语音单元（如单词或音节），通过拼接这些单元生成语音。这种方法生成的语音自然度高，但需要大量的录音数据，且难以控制语音的韵律和情感。
- 2.统计参数语音合成（SPSS）：
文本分析：将文本转换为语言特征，包括音素、时长、基频等。
声学模型：使用 HMM 或 DNN 模型预测声学特征（如梅尔倒谱系数、线性谱图等）。
声码器：将声学特征转换为波形，常用的声码器包括 STRAIGHT [155] 和 WORLD [238]。
改进：通过引入深度学习（如 DNN、RNN），提高了合成语音的自然度和可懂度。
- 3.神经网络语音合成（Neural TTS）：
文本分析：简化了文本分析模块，直接使用字符或音素作为输入。
声学模型：使用神经网络（如 Tacotron 1/2 [382, 303]、Deep Voice 3 [270]）生成梅尔谱图或其他声学特征。
声码器：使用神经声码器（如 WaveNet [254]、WaveRNN [150]）直接从声学特征生成波形。
改进：通过引入注意力机制和自注意力机制，进一步提高了合成语音的自然度和鲁棒性。
- 4.端到端语音合成：
直接从文本到波形：省去了中间的声学特征转换步骤，直接从文本生成波形。
代表性模型：WaveNet [254]、Tacotron 2 [303]、FastSpeech 1/2 [290, 292] 等。
改进：通过引入非自回归生成和并行生成技术，进一步提高了生成速度和效率。
- 5.完全端到端语音合成：
直接从文本到波形：进一步简化了模型结构，直接从文本生成波形，省去了中间的声学特征和梅尔谱图转换。
代表性模型：ClariNet [269]、FastSpeech 2s [292]、EATS [69] 等。
改进：通过引入更高效的生成模型和优化技术，进一步提高了生成速度和语音质量。

### 语音合成分支方向

当然可以，以下是对Fast TTS、Low-Resource TTS、Robust TTS、Expressive TTS和Adaptive TTS的中文解释：
**Fast TTS（快速文本到语音）**
Fast TTS 旨在提高文本到语音合成系统的速度。传统的TTS模型通常使用自回归生成方法，这对于长序列来说非常慢。Fast TTS技术通过以下方法来加速推理过程：
非自回归生成：像FastSpeech和FastSpeech 2这样的模型使用前馈网络并行生成梅尔谱图，显著减少了推理时间。
轻量级模型：通过模型剪枝、量化和知识蒸馏等技术减少计算成本和内存占用，使TTS模型适合部署在移动和嵌入式设备上。
领域知识集成：利用线性预测和子带建模等方法加速波形生成。
**Low-Resource TTS（低资源文本到语音）**
Low-Resource TTS 解决了在训练数据有限的情况下构建高质量TTS系统的挑战。这对于低资源语言或说话者尤为重要。关键技术包括：
自监督训练：利用未配对的文本或语音数据通过预训练方法（如BERT）增强语言理解或语音生成能力。
跨语言迁移：在高资源语言上预训练TTS模型，然后在低资源语言上微调，以利用共享的语言特征。
跨说话者迁移：使用语音转换将一个说话者的模型适应到另一个说话者，即使数据有限。
语音链/反向转换：结合TTS和ASR系统，使用未配对的文本和语音数据相互改进。
**Robust TTS（鲁棒文本到语音）**
Robust TTS 旨在提高TTS系统的可靠性和稳定性，确保即使在挑战性场景下也能生成正确且可理解的语音。鲁棒性问题通常源于：
对齐学习：学习文本和语音之间的对齐困难，导致跳字、重复等问题。增强鲁棒性的技术包括：
增强注意力机制：使用基于内容、位置或混合的注意力机制，确保局部、单调和完整的对齐。
用时长预测替代注意力：显式预测每个音素或字符的时长，以弥合文本和语音序列之间的长度差异。
暴露偏差和错误传播：自回归生成中的问题，错误在推理过程中累积。解决这些问题的方法包括：
教授者强制：对齐训练和推理数据的分布。
双向正则化：使用从左到右和从右到左的生成来减少错误传播。
**Expressive TTS（表现力文本到语音）**
Expressive TTS 专注于生成不仅可理解而且自然、富有情感的语音。这涉及建模和控制语音的各个方面，例如：
韵律、风格和情感：捕捉和控制语音的语调、重音、节奏和情感内容。
解耦和控制变化信息：使用参考编码器、变分自编码器（VAE）和对抗训练等技术解耦和控制语音的不同方面，如说话者音色、韵律和噪声。
先进生成模型：利用GAN、流和扩散模型等强大生成模型更好地捕捉语音的多模态性质，提高表现力。
**Adaptive TTS（自适应文本到语音）**
Adaptive TTS 使TTS系统能够用最少的数据和计算资源适应新的说话者或风格。这对于定制语音应用至关重要。关键技术包括：
通用适应：通过说话者增强和跨语言适应等方法提高源TTS模型的泛化能力。
高效适应：减少每个目标说话者所需的适应数据和参数。技术包括：
少样本适应：仅用几分钟或几秒钟的配对文本和语音数据进行适应。
未转录数据适应：使用未转录的语音数据进行适应，通过语音重建和潜在对齐。
零样本适应：在没有适应数据的情况下使用说话者嵌入和预训练模型进行适应。
这些TTS的高级主题旨在推动语音合成技术的边界，使其更高效、更鲁棒、更具表现力，并能适应各种场景和用户需求。

## 论文5——Neural voice cloning with a few samples

本文标题为“Neural Voice Cloning with a Few Samples”，主要研究了如何通过少量的音频样本实现个体语音的克隆。文章介绍了两种神经语音克隆方法：说话人自适应（speaker adaptation）和说话人编码（speaker encoding）。自适应方法基于微调多说话人生成模型，而编码方法基于训练一个单独的模型直接推断新说话人的嵌入，应用于多说话人生成模型。实验表明，两种方法都能在有限的样本下实现较高的语音自然度和说话人相似度，尽管自适应方法在自然度和相似度方面略优，编码方法在克隆时间和所需内存方面显著较少，更适用于资源受限的部署环境[3]。
文章首先介绍了深度神经网络在图像生成、语音合成和语言建模等领域的成功应用，并指出这些网络能够对复杂数据分布进行建模，并且可以通过外部输入控制生成样本的内容和风格[4]。接着，讨论了语音合成中的生成模型，强调了说话人身份在捕获声音特征（如音高、语速和口音）的重要性[5]。文章还提出了基于神经网络的语音克隆系统的挑战，即在有限样本情况下学习并泛化到未见文本的能力[5]。
在相关工作部分，文章详细讨论了神经语音合成[10]、少样本生成模型[11]、说话人依赖语音处理[12]和声音转换[13]等领域的进展。此外，介绍了多说话人生成模型的构建，以及如何通过说话人嵌入优化[14]。研究者还设计了实验部分，包括数据集的选择[28]、模型规格[29]和语音克隆性能评估[33]。

## 网站6—— State of the art in Voice Cloning: A review

低资源多语言零样本多说话人TTS

