今天啊就让我们一起用Transformers
来研究Transformer
破解其中变形的奥秘
Transformer网络
是谷歌在2017年提出的著名算法
在机器翻译上呢
效果巨好
近年来啊
横扫NLP领域
获得了极大的关注
但是其复杂的结构
往往让很多初学者头疼不已
输入一句中文
经过Transformer怎么就变成了英文呢
让我们钻进他的肚子里
看看其中有何精巧之处
总的来说啊
它分为编码和解码两个过程
英文就是incoder和decoder
这就很像变形金刚了
先拆解成流体一般的变形元素
然后再重新组装
变回机器人吧
每个引coder和decoder
又是一个串联的组合
比如经常见到的6层结构啊
这可以理解为啊
一次完整的变形
都需要重复2乘以6共12步操作
每个incoder呢
又包含了self attention和潜窥网络
两个核心模块儿
上期我们讲过tension机制哈
本质上就是通过甲醛求和
获得对上下文的全局感知
通俗一点说啊
self attention就是变形金刚的拆解对照表
算算各个零部件的权重
标明互相间的关系
而乾坤网络呢
就是根据这些权重变义词形状
decoder除了self attention和潜窥网络
还多了一层
叫做encoder
decoder attention
其作用就是在组装的时候
不光要考虑自己
还要兼顾拆解的时候的整体信息
落实到机器翻译上
在解码的时候
每个词啊
不光要看已经翻译的内容
还要考虑in coder中上下文的信息
每个self attention呢
又会分解为多个部分
这就是传说中的Multi head attention
这像极了变形金刚的拆解过程啊
通常也是按照头
身子胳膊
腿和手啊等8个部分分别进行
那么把刚才这些模块组织起来
就是Transformer的完整结构了
这张图看着特别复杂
是吧非常吓人
但其实呢
就是变形金刚的拆解组装过程图
好了网络结构清楚了
现在让我们看看
数据是如何在这些模块间流动的
首先用算法把单词向量化
切入位置信息变成统一长度
比如都是512位
然后引coder把它作为输入
通过self attention和前馈网络
发送到下一个编码器
其中self attention就是一个零件自查表
通过权重标明相互关系
嵌入上下文信息
具体是怎么算的呢
每个输入项量先嵌入位置信息
然后分别乘以3个训练好的向量q
k和v再用每个单词的q向量
和所有单词的k向量相乘
得到的权重就是个tens
再然后呢
通过归一化
用Softmax函数过滤掉不相干的单词
乘以微向量后
加权求和
就得到了输出向量z
这个过程中啊
QKV三个向量起到的作用
非常像数据库中的Perry
k和value三步操作
所以呢
人们就给他起了这样古怪的名字
其实
这些细节开始不必眉毛胡子一把抓
只要知道经过一系列矩阵操作
实现了单词间的权重计算就行了
用矩阵语言来描述上述过程
要简洁许多
先用输入单词矩阵x
分别乘以3个训练好的权重矩阵
得到QTV矩阵
然后用下面的公式计算self attention
和x相比呢
输出矩阵z的维度没有变
只是其中被编码
掺入了其他单词的上下文信息
在Matihead探审中
使用了不同的权重矩阵QKV
进行了8次计算
你可能会问
费这么大劲干什么呢
一次不就够了吗
这样做的主要目的啊
是为了消除QKV初始值的影响
说白了就是一件事
找8个人做
万一哪个不靠谱
也不影响大局是吧
最后呢再求一个加权平均
合成一个z
把上述过程合起来
它就是长这个样子的
讲到这你应该明白
Transformer最核心的self attention机制了吧
无非就是通过一系列骚操作
求关联权重
Transformer网络细节很多
记不住啊
没关系咱们梳理一下
如同变形金刚
变形过程包含编解码两个部分
先打散成零件
再组装每个过程呢
又分为6步
每步又按照不同的初始值分为了8块
每块都有自己的变形说明书
记录了各个零件的权重
和与其他零件的相互关系
这就是self的探审机制了
看着网上
纷繁复杂的Transformer介绍
一定好奇
作者当年是如何脑洞大开想出来的
其实啊
就和我们小时候玩变形金刚是一样的
或许作者当年的灵感就来自于这
最后让咱们看看
各个模块间是如何协同工作的
编码器处理输入序列
将输出转化为attention向量k和v
也就是我们所说的零件拆解说明书
解码器阶段呢
一边看自己的拆解说明书
一边考虑拆分时
与其他零件的相互关系
每步组装输出一个零件
重复这个步骤就完成了
变形解码器的输出是个项链
它如何变成一个单词呢
这就是最后线性层和Softmax层的工作
其中线性层
它是一个简单的全连接网络
把解码器输出投影成一个一维项量
而这个项量的维度很长
包含了所有可能出现的单词总和
比方说是2万
Softmax层呢
进一步归化其中概率最高的单词
就是最后的输出
这样就很容易理解了哈
Transformer网络的训练
依然沿用了梯度下降算法
通过减小预测和实际值之间的误差
进行反向传播来调整模型参数的权重
实现最后的学习效果
好了以上就是本期的内容
希望对你快速理解Transformer有所帮助
谢谢大家
