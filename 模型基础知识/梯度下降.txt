咱们就来聊聊梯度下降法是个什么鬼
part one一度下降的基本思想老规矩
咱们先从一个故事开始
话说有三个兄弟
被困在了浓雾弥漫的山上
渴得要死
他们的目标
是看谁能尽快到山谷中找到水源
由于大雾能见度极低
难以确定下山的路径
只能一点点探路决定前进的方向
老大非常谨慎
四下综合比较后才选择最陡的方向
老二随性胆大
随便探一下就朝较低处走去
老三中庸普通
没有大哥小心翼翼
也没有二哥大大咧咧
探测几下就走最陡峭的方向
你觉得
谁最有可能先到山脚找到水源呢
听完这节课
你就会知道答案
三兄弟下山找水的过程
都用到了梯度下降
每走一段路
测量出最陡的方向
然后向前重复这个过程
就能成功抵达山
在这个故事中
大山代表了什么
又该如何确定梯度呢part two
梯度下降法的主要原理
一确定一个小目标
预测函数机器学习的一个常见任务
是通过学习算法
自动发现数据背后的规律
不断改进模型
然后做出预测
为便于理解
我们举一个简单的例子
在二维直角坐标系中
有一群样本点
横纵坐标
分别代表一组有因果关系的变量
比如房子的价格和面积
人的身高和不符等等
常识告诉我们
它们的分布是正比例的
也就是一条过圆点的直线y等于w x
我们的任务就是设计一个算法
让机器能够拟合这些数据
帮助我们算出直线的参数w
一个简单的办法
就是先随机选一条过圆点的直线
然后计算所有样本点和它的偏离程度
再根据误差大小来调整直线的斜率w
在这个问题中
直线y等于w乘x
就是所谓的预测函数
二找到差距代价函数原理我们知道了
如何让计算机实现呢
首先我们需要量化数据的偏离程度
也就是误差
最常见的方法是均方误差
顾名思义
就是误差平方和的平均值
我们先看一个点P1
它的坐标是X1
Y1 对应的误差是E1
它等于这个点的真值
Y1与预测值w乘X1的差的平方
用完全平方公式展开就是这样
同理点P2
P3一直到PN的误差E2 E3
EN也都是一样的形式
我们的目的是求所有点误差的平均值
考虑到x y和样本数n都是未知数
因此通过合并同类项
然后用常量a b
c分别代替不同项的系数
我们可以大大简化最终的式子
如此以来
就得到了一个
高中学过的一元二次函数
数学是不是很美
一通推倒之后居然变得如此简单
这个误差函数
表示了学习所需要付出的代价
因此常常被称为代价函数
如果高中数学还没忘光的话
你应该还记得
因为二次项系数a是x的平方和大于0
所以这个函数
对应着一个开口向上的抛物线图像
当w的取值发生变化时
直线绕圆点旋转
对应到抛物线图像
就是取值点沿着曲线的运动
通过定义预测函数
然后根据误差公式推导代价函数
我们成功地将样本点拟合过程
映射到了一个函数图像上
三明确搜索方向
梯度计算找到了代价函数图像
我们该怎么走呢
机器学习的目标
是拟合出最接近训练数据分布的直线
也就是找到使得误差代价最小的参数
w对应在代价函数图像上
就是它的最低点
这个寻找最低点的过程
就是梯度下降要干的活
假定起始点在曲线上任意一处
直觉告诉我们
只要选择向陡峭程度最大的方向走
就能更快到达最低点
这个陡峭长度就是梯度
英文是gradient
它是代价函数的导数
对抛物线而言
就是曲线斜率
4大胆的往前走吗
学习率确定方向以后
就要前进了
但步子该迈多大呢
假如随便选一个数
比如0.1 算法的效果是这样的
可以看到它一直在最低点附近震荡
难以收敛
如果直接用斜率值作为补偿
怎么样离最低点
远时斜率大
可以快速收敛
离最低点近时
斜率越小
收敛的就越精准
听上去不错的点子
可是实际效果却是左右反复横跳
依然无法收敛到最小值
看样子还是步子迈的太大
我们把斜率缩小试试
我们让斜率乘以一个非常小的值
比如0.01 再来看看效果
哈哈下降得如此顺滑
连德芙都自叹不如啊
这个很小的值有一个好听的名字
叫做学习率
通过学习调整权重的方式
就是新w等于旧w
减去斜率乘以学习率
5不达目标不罢休
循环迭代
总结起来
梯度下降法的完整过程包括一
定义代价函数
2选择起始点3计算梯度
4沿着这个方向按照学习率前进
5重复第三第四步
直到找到最低点
这个流程就是所谓的梯度下降算法
代价函数起始点梯度学习率
这些都是梯度下降法的核心要素
也是后来各种算法改进的重要方向
part three实际情况没有这么简单
看到这里
你可能会有些疑问
既然都知道了
代价函数就是一个一元二次的抛物线
为什么不用数学手段直接求解呢
这种求最大值最小值的问题
不是中学数学常见题型吗
大家没有印象的话
请向数学老师道歉
这是因为实际问题中
序列样本的分布千奇百怪
代价函数也可能千变万化
不太可能是一条简单的抛物线
比如我们的预测函数稍作改动
变成y等于w
x加b那么代价函数就变成了误差异
关于两个参数w和b的曲面
这依然是比较简单的情形
因为只有一个最小点
代价函数还可能会是一条波浪线
当有多个最小点存在时
机器学习的目标将是找到最低的那个
也就是所谓的全局最优
而不是局部最优
代价函数
也可能会是一个起伏不定的曲面
又或者某种无法用三维图像描述的
更复杂函数
例如房价
除了和面积相关外
还和城市地段
朝向政策等等各种因素相关
这个问题当中
代价函数变成10维百维
都有可能将很难可视化的展示出来
但无论有多少维度
都可以通过梯度下降法
来寻找误差最小的点
part four梯度下降法的各种变体
现在回到我们最初的问题
三个兄弟
到底谁更有可能先找到水源呢
实际上他们分别代表了
三种不同类型的梯度下降算法
大哥小心翼翼
每次都把四周探测的明明白白的
做法就像批量梯度下降法
简称BGD它的下降过程就像这样
左侧是样本点
右侧是用等高线表示的代价函数曲面
可以看到
它的运算是用全部训练样本参与计算
梯度下降的非常平稳
走出了一条强迫症一般的漂亮曲线
他是梯度下降最原始的形式
好处是能够保证算法的精准度
找到全局最优解
但却让训练搜索过程变得很慢
代价很大
大哥的下山路线可能会是这样的
虽然速度很慢
但是稳如老狗有木有
老二大大咧咧探测一下就走的做法
就像随机梯度
下降法简称SGD
它的下降过程
顾名思义
非常随性
每下降一步
只需用一个样本进行计算
它的行进路线就像个醉汉
深一脚浅一脚的前进
虽然大方向没错
但下降的非常不平稳
它的好处是提升了计算的速度
但是却牺牲了一定的精准度
采用这种方法
老二的下山路线可能是这样的
速度虽然是快了
却像没头苍蝇一样乱闯
老三结合了老大老二的优点
试探几下
果断就走
这种方法叫做小批量梯度下降法
简称MBGD
每下降一步
选用一小批样本参与计算
它的下降过程虽然没有大哥平稳
有规律但是快的多
虽然没有二哥速度快
但准确了很多
走出了一条简洁高效的路线
采用这种方法
老三的下山路线可能是这样的
又快又稳
有没有
相对来说
老三的方法最科学
我国的著名数学家华罗庚先生
曾经提出过一种最优化方法
叫瞎子爬山法
和老三的行为非常像
除了一个是上山
一个是下山
这个方法还有个别名
叫最速下降法
算法效果从名字上就可见一斑
梯度下降法简单有效
适用范围广
但也并非完美无缺
比如前面讲过
他对学习率的设定非常敏感
学习率太大
你可能会反复横跳
找不到最低点
学习率太小
又会浪费很多的计算量
另一个问题就是
除了效率极低的DGD外
你无法保证找到全局最低点
很可能的结果是陷入局部低点
难以自拔
就像随性的老二下到半山腰
找到个小
水湾还以为自己发现了太平洋
经常爬山的三兄弟为了解决这些问题
也在不断研究新的下山方法
比如动态调节学习率的阿达贵
经常更新的参数学习率就小一些
不常更新的学习率大一些
这种方法的一个问题是
频繁更新参数的学习率有可能会过小
以致逐渐消失
因此
就出现了解决这一问题的RMS Pro算法
以及更高级的
不需要设置学习率的Ada Delta算法
还有
融合了adagrid和RMS Pro算法的item算法
MOMENTUM算法在下降过程中
充分考虑前一阶段下降的惯性
这个方法的路线有点像滚下山的样子
此外还有更复杂的FTRL等方法
这些艰深的内容
不是我们这次课的重点
感兴趣的同学
可以期待一波我们后续的课程
以上就是今天的内容了
如果觉得这个视频有所帮助
欢迎三连
您的支持
将是我们持续更新的最大动力
