昨晚上睡不着
12点半看了一眼b站上千LSTM视频
居然有40人同时在看
哎呀我这心里感动的呀
都说现在的年轻人爱躺平
那是他们不知道
居然有这么多人在被窝里熬夜学着AI
得嘞今儿啊
给大家上点福利
聊聊这两年
最时髦的attention机制是个啥
废话少说
咱们先看视频
I
回顾刚才的画面
请把您心中的最爱打在公屏上
看见这些权重数字没
attention就是权重
权重就是attention
是不是有点猝不及防
要的就是这效果
从现在起
请记住刚才这句话
attention机制的重要性就不用我说了吧
Transformer Bert
GPT等等这几年最流行的AI模型
全是在它的基础上发展起来的
来咱们一起看看在机器学习里
它是怎么用的
以AI翻译为例
我们输入一句话
机器是如何做到通过上下文动态
改变翻译结果的呢
秘诀就在Tension机制
只考虑自己的权重
games本意是游戏
考虑北京时
它被理解为比赛
当综合winter和2022时
翻译成冬奥会就更准确了
是不是很神奇
让我们看看这种机制它是怎么实现的
RNN模型改进了传统神经网络
建立了网络隐藏间的持续关联
每一时刻的隐藏状态ST
不仅取决于输入XT
还包含了上一时刻状态ST减1
两个RN结构组合
形成引coder底coder模型
先对一句话编码
然后再解码
就能实现机器翻译
但是这种不管输入多长都统
一压缩成相同长度
编码c的做法眉毛胡子一把抓
会导致翻译精度下降
attention机制呢
就是通过每个时间输入不同的c
来解决这个问题
其中系数阿尔法体i
标明了在t时刻所有输入的权重
以CT的视角看过去
在他眼中就是不同输入的注意力
因此呢也被称为attention
分布网络结构定了
我们可以通过神经网络数据训练
得到最好的探神权重矩阵
通过Tension机制的引入
我们打破了只能利用Incoder
形成单一向量的限制
让每一时刻
模型都能动态地看到全局信息
将注意力集中到对当前单词
翻译最重要的信息上
大大改善了机器翻译的效果
后来啊
随着GPU等大规模并行运算的发展
人们发现RN的顺序结构很不方便
难以并行运算
效率太低
既然额摊膳模型本身
都已经对全部输入进行了打分
RN中的顺序好像没什么卵用
还不如干脆简化掉
这就是传说中的self attention机制了
用刚开始的例子来表示
就是去掉了输入的箭头
in coder编码阶段
利用attention机制
计算每个单词
与其他所有单词之间的关联
比如当翻译games时
北京winter 2022都较高地产生score
利用这些权重加权表示
再放到一个所谓的前溃神经网络中
得到新的表示
就很好地嵌入了上下文的信息
这样的步骤重复几次
效果会更好
这个过程啊
它有点像照镜子
你想在别人眼中留下好的印象
最好出门前先自己多照几次镜子
每次上下全身打量一下
不同部位呢
给予不同的权重
您到底是想突出靓丽的脸蛋
傲人的上身
还是雪白的大长腿
哎这就是self探身的真谛了
decoder的时候啊
也是类似
不仅要看之前产生的输出
而且呢还得看incoder得到的输出
换句话说
你在别人心中的印象
不仅受之于你今天的打扮啊
这个incoder
还来自于他以前对你的印象
人类的视觉系统就是一种attention机制
他将有限的注意力集中在重点信息上
从而节省资源
快速获得最有效的信息
而Tension机制在AI领域的应用
最早呢也是在计算机视觉领域
但真正的发扬光大是在NRP领域
2017年 谷歌的Transformer算法横空出世
引发了大家对attention机制的关注
这个咱们下期再讲
2018年Bert和GPT算法效果出奇的好
进而让attention机制越发的走红
其实啊总结起来的
产生机制有三大优点参数更少
速度更快
效果呢更好
过去5年多
attention遍地开花
但不管怎么变
它的核心思想其实很简单
通过加权求和解决
context的理解
本质上就是在不同的上下文下
专注不同的信息
因此无论NLP图像还是搜索推荐
任何有类似需求的地方
都能让他探神
大展拳脚
这是一个分心的时代
高速的互联网
各种手机APP
让人沉迷的游戏娱乐搞笑的短视频
无时无刻都在冲击着我
们的大脑
突然之间你会发现
在这个互联网时代
你已经很难专注于做一件事情
锲而舍之
朽木不折
锲而不舍
金石可镂
聚焦他是非常宝贵的一项天赋
而贪神力量之强大
它能把一个人的潜力发挥到极致
能达到金字塔顶端的动物有两种
苍蝇和蜗牛
像苍蝇一样于众生中脱颖而出者
实数少数
但即使是蜗牛
坚持不懈
一样可以做出难以想象的成就
好了今天咱们就讲正多
谢谢大家
