# 语音克隆

打标需要文字与语音对应，我们实现的功能是无文本语音输入还是有文本？

## 1.语音合成技术综述及研究现状_魏伟华

综述了语音合成技术的实现方法和近年的发展情况。
基本如下：
语音技术分为语音识别和语音合成，STT和TTS，前者困难在于说话人的口语、语种、环境等影响很大；后者困难在于需要高质量的语音库打标，耗时耗力。
TTS基本原理可以是用数据训练模型和规律规则制定，前者对语料库要求高，后者要求对语言学有了解。

关键技术：
共振峰合成：不同的音色有不同的共振峰，提取频率和带宽参数
波形拼接：语音单元做变频等处理后重叠相加，类似vocal void术力口
谐波加噪声：信号分成谐波加噪声，在合成时进行高低频分离处理
深度学习：深度神经网络DNN，多层神经网络MLP，卷积网络

新应用：
语音克隆：语音编码或者语音合成，前者训练模型，后者微调

术语了解：
滤波器，共振峰滤波器，高频滤波器
端到端语音合成Tacotron模型，转换成波形的算法较差。结合Wavenet做两阶段更好
梅尔频谱，时间声学特征转换为频率，应用梅尔尺度作划分

## 2.基于音色一致的语音克隆说话人特征提取方法_李嘉欣

语音合成［1］是依据文本合成对应语音的技术。在深度学习技术发展之前，拼接语音合成［2］和统计参数语音合成方法［3］是主流语音合成技术。在过去几年中，随着深度学习技术的发展，基于神经网络强大的非线性建模能力，神经语音合成模型生成语音的质量、自然度和可懂度都有了很大的提高
语音合成：
现阶段神经语音合成常用两阶段语音合成方法（2022年10月），分为文本到特征频谱的声学预测模块，频谱到波形的声码器模块。前者模型如 Tacotron2［4］和 FastSpeech2［5］，后者模型如 WaveNet［6］，WaveGlow［7］，HiFi-GAN［8］。
完全端到端比如VITS模型更加先进。
语音克隆：面向小语料的个性化语音合成，不需要大量的高质量语音文本对数据做端到端训练。
语音克隆方法：
说话人自适应：用说话人语音微调（重新训练）多说话人语音合成模型，防止过度拟合可以拆分为两个级联模块，用音素后验概率图做中间特征相连。AdaSpeech［12］在 Fast‐Speech2 模型的基础上引入条件层归一化
说话人编码：训练一个单独的说话人编码模块，用来提取声学特征向量，再传递给模型生成波形。预训练说话人编码器来自其他语音任务的特征提取模块，比如语音验证等，可能对齐困难。参考编码器方法将编码器和语音合成模型联合训练，？？？？？？
针对说话人编码方法，关键在于提取声学特征，本文提出基于音色一致性约束损失的训练方法，减少杂音噪声，针对未见说话人做出提高。
该文献使用TitaNet为基础，加入音色一致性约束，再使用VITS合成波形。

## 3.面向小样本的个性化中文语音合成研究_杨宜涛

研究小样本细颗粒说话人特征，基于元学习多音字消歧
语音克隆方法：使用TitaNet作为说话人编码器的基本架构，引入音色一致性约束损失，提取更精确的说话人音色特征。
多音字消歧方法：基于匹配网络的对比策略，不再将多音字消歧视为分类任务，而是通过对比不同读音下的语义特征来确定多音字的发音。

### 模型

这段文本详细介绍了深度学习技术在语音合成系统中的应用和发展，提到了多个重要的模型和系统，以下是这些模型的总结：

1. **基于DNN或RNN的声学模型**：
   - 早期深度语音合成系统中，深度网络主要用于声学建模部分，将文本特征转换为声学特征。

2. **Wavenet模型**：
   - 由Van等人在2016年提出，采用自回归生成模式，利用因果卷积和空洞卷积逐点生成音频信号，实现从文本特征到声音波形的直接转换。

3. **Deep Voice语音合成系统**：
   - Arik等人基于Wavenet提出，对Wavenet网络结构进行优化，实现文本到声音波形的高质量实时合成。

4. **Char2wav端到端语音合成系统**：
   - Sotelo等人提出，包含两个独立训练的网络：一个序列建模网络将文本转换为声学特征，另一个声码器将声学特征还原为声音波形。

5. **Tacotron端到端语音合成系统**：
   - Wang等人提出，设计了一个声学模型网络，输入文本输出梅尔频谱参数，再利用Griffin-Lim声码器将频谱参数还原为声音波形。

6. **Tacotron2**：
   - 在Tacotron基础上改进，使用Wavenet声码器替换Griffin-Lim声码器，提高了语音的清晰度和自然度。

7. **Transformer TTS**：
   - 基于Transformer架构的端到端语音合成模型，由Li等人提出，训练速度快于Tacotron2。

8. **FastSpeech**：
   - 由Ren等人提出，基于非自回归的Seq2Seq模型，减少了由于注意力对齐问题产生的复读、漏读等情况。

9. **FastSpeech2**：
   - 在FastSpeech基础上改进，简化了音素时长预测器的训练过程，提高了模型训练的便捷程度。

10. **Parallel Wavenet**：
    - Oord等人提出，使用知识蒸馏思想和标准化流模型，提高了Wavenet声码器的运算速度。

11. **WaveGlow**：
    - Prenger等人提出，基于流模型的非自回归深度网络声码器。

12. **WaveRNN**：
    - Kalchbrenner等人提出，单层GRU网络结构，实现了合成质量与合成效率之间的平衡。

13. **HifiGAN声码器**：
    - Kong等人提出，基于对抗训练的神经网络声码器，包括生成器网络和判别器网络，实现了高效率和高质量的语音合成。

这些模型和技术的发展，展示了深度学习在语音合成领域的重要进展，从早期的基于统计参数的合成系统，到后来的端到端合成系统，以及声码器技术的不断革新，都极大地推动了语音合成技术的发展。

### 模型解释

1. **自回归与非自回归**
自回归模型（Autoregressive Model）是一种基于条件概率的生成模型，它通过逐步生成序列中的每个元素来构建整个序列。在语音合成中，这意味着当前的音频样本依赖于之前生成的样本。自回归模型如Tacotron2和Transformer TTS被广泛使用。这些模型能够生成连贯、自然的语音，但计算复杂度较高，需要较长的训练时间和较大的计算资源。
非自回归模型（Non-Autoregressive Model）与自回归模型的主要区别在于，它能够并行生成整个序列，而不是逐个元素。这种模型通过消除输出标记之间的时间依赖性，在一个推理步骤中预测整个输出标记。非自回归模型如FastSpeech和FastSpeech2在语音合成中得到了应用。这些模型通过引入时长预测器和方差预测器来帮助文本与频谱进行对齐，从而减少训练的时长和减少漏词、重复发音问题。

2. **自回归模型的应用示例：**
Tacotron2：使用编码器-解码器结构，以自回归生成方式将文本转化为梅尔频谱，再借用GriffinLim声码器进行声音波形的合成。
Transformer TTS：将输入转化为更加细粒度的音素输入来训练模型，结合WaveNet声码器解码生成声音波形。

3. **非自回归模型的应用示例：**
FastSpeech：引入时长预测器帮助文本与频谱进行对齐，采用教师学生模型辅助训练合成梅尔频谱。
FastSpeech2：引入方差预测器为生成语音提供尽可能多的参数信息，帮助解决语音合成任务中一对多的问题。

## 4.小结

在知网找到的文献提及的模型都是2023年及以前，而语音克隆领域进步飞速。
要进行语音克隆说话人的音色，当前常用的模型和方法主要包括以下几种：

### 常用模型

1. **GPT-SoVITS**
   - 这是一个强大的音色克隆模型，支持少量语音转换和文本到语音（TTS）。只需提供**5秒**的语音样本即可实现80%~95%的声音相似度，提供1分钟样本可以更接近真人效果。该模型支持中文、英文、日文等多种语言的推理，并集成了多种辅助工具，如语音伴奏分离和自动训练集分割等【^1^】。

2. **CosyVoice**
   - 由阿里通义实验室发布，CosyVoice专注于自然语音生成，支持多语言和情感控制。仅需**3~10秒**的原始音频即可生成模拟音色，支持中英日粤韩五种语言的生成，效果显著优于传统语音生成模型【^2^】。

3. **F5-TTS**
   - 由上海交通大学开发，F5-TTS是一款高性能的文本到语音系统，采用流匹配非自回归生成方法。该系统支持多语言合成，能够快速生成自然流畅的语音，适用于实时语音生成和高效语音克隆任务【^6^】【^13^】。

4. **FishSpeech1.4**
   - 这是一个开源的语音克隆模型，支持多种语言，能够在**10到30秒**的语音样本基础上生成高质量的TTS输出。该模型具有强大的泛化能力，不依赖于音素进行TTS，能够处理任何语言脚本的文本【^5^】【^14^】。

5. **MaskGCT**
   - 这是一个**完全非自回归**的TTS模型，消除了对文本和语音监督之间显式对齐信息的需求，能够在无额外监督条件下实现零样本学习，迅速生成自然流畅的语音【^5^】【^15^】。

### 使用方法与部署

- **GPT-SoVITS的部署**：
  1. 安装依赖并下载项目。
  2. 通过WebUI或命令行接口进行推理，生成目标语音。

- **CosyVoice的部署**：
  1. 在阿里云的Serverless平台上创建应用。
  2. 通过API调用进行声音克隆和情感控制。
  3. 提供参考音频和文本，生成合成语音。

- **F5-TTS的部署**：
  1. 创建Python虚拟环境并安装所需依赖。
  2. 使用Gradio App或命令行接口进行推理。

- **FishSpeech1.4的部署**：
  1. 安装Python环境并依赖库。
  2. 使用命令行接口或WebUI进行语音克隆。

- **MaskGCT的部署**：
  1. 通过Docker或直接在本地环境中安装。
  2. 使用命令行接口进行推理，生成目标语音。
